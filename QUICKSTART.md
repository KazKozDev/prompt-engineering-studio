# PE Studio - Quick Start Guide

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

PE Studio —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:

1. **Backend API** (FastAPI) - –ø–æ—Ä—Ç 8000
2. **Frontend** (React + Vite) - –ø–æ—Ä—Ç 5174
3. **LLM Clients** (Ollama, Gemini, OpenAI)

## –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# Backend
pip install -r requirements.txt

# Frontend
cd frontend
npm install
cd ..
```

### 2. –ó–∞–ø—É—Å–∫ Backend API

```bash
PYTHONPATH=/Users/artemk/prompt-engineering-studio python src/api_server.py
```

API –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞: `http://localhost:8000`

### 3. –ó–∞–ø—É—Å–∫ Frontend

```bash
cd frontend
npm run dev
```

Frontend –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞: `http://localhost:5174`

## –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã

### ‚úÖ –ì–æ—Ç–æ–≤—ã–µ —Ä–∞–∑–¥–µ–ª—ã:

- **Studio** - –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
  - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
  - –í—ã–±–æ—Ä —Ç–µ—Ö–Ω–∏–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
  - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Ollama, Gemini, OpenAI
  - API endpoints: `/api/techniques`, `/api/generate`, `/api/models/{provider}`

### üöß –í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ:

- **History** - –∏—Å—Ç–æ—Ä–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–π (placeholder: `/api/history`)
- **Templates** - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã (placeholder: `/api/templates`)
- **Datasets** - –¥–∞—Ç–∞—Å–µ—Ç—ã (placeholder: `/api/datasets`)
- **API Keys** - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–∞–º–∏ (placeholder: `/api/settings`)
- **Settings** - –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (placeholder: `/api/settings`)

## API Endpoints

### GET /api/techniques
–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### GET /api/models/{provider}
–ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (ollama, gemini, openai)

### POST /api/generate
–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã

**Request:**
```json
{
  "prompt": "Your prompt here",
  "provider": "ollama",
  "model": "llama2",
  "api_key": "optional_for_gemini_openai",
  "techniques": ["cot", "react", "tree_of_thoughts"]
}
```

**Response:**
```json
{
  "results": [
    {
      "technique": {...},
      "response": "Optimized prompt",
      "tokens": 150
    }
  ]
}
```

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
prompt-engineering-studio/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api_server.py          # FastAPI —Å–µ—Ä–≤–µ—Ä
‚îÇ   ‚îú‚îÄ‚îÄ llm/                   # LLM –∫–ª–∏–µ–Ω—Ç—ã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base.py
‚îÇ   ‚îú‚îÄ‚îÄ prompts/               # –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manager.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # –£—Ç–∏–ª–∏—Ç—ã
‚îÇ       ‚îî‚îÄ‚îÄ logger.py
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ App.tsx            # –ì–ª–∞–≤–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
‚îÇ       ‚îú‚îÄ‚îÄ services/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ api.ts         # API –∫–ª–∏–µ–Ω—Ç
‚îÇ       ‚îî‚îÄ‚îÄ index.css
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml      # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
‚îî‚îÄ‚îÄ requirements.txt
```

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ `config/model_config.yaml` –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–µ–π:

```yaml
models:
  ollama:
    model_name: llama2
    base_url: http://localhost:11434
  
  gemini:
    model_name: gemini-pro
  
  openai:
    model_name: gpt-4
```

## Troubleshooting

### Backend –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: `pip install -r requirements.txt`
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –ø–æ—Ä—Ç 8000 —Å–≤–æ–±–æ–¥–µ–Ω

### Frontend –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ API
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ backend –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É 8000
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ `src/api_server.py`

### Ollama –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: `ollama serve`
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: `curl http://localhost:11434/api/tags`
